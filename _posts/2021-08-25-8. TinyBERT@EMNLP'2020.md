---
layout: post
title: "8. Paper Review: TinyBERT: Distilling BERT for Natural Language Understanding@EMNLP'2020"
# date: 2016-06-19 10:00:00 +0900
categories: review
# tags: [LSTM, Anomaly Detection, ICML, Deep Learning]
---
[Paper Link](https://aclanthology.org/2020.findings-emnlp.372.pdf)

# Abstract
BERT와 같은 언어 모델 사전 학습은 자연어 처리(NLP) 작업의 성능을 크게 향상시켰다. 그러나 사전 학습된 언어 모델은 계산 비용이 많이 들어 자원 제약이 있는 장치에서 효율적으로 실행하기 어렵다. 이에 정확도를 유지하면서 추론 속도를 높이고 모델 크기를 줄이기 위해, 트랜스포머(Transformer) 기반 모델의 지식 증류(Knowledge Distillation, KD)를 위해 특별히 설계된 새로운 트랜스포머 증류(Transformer distillation) 방법을 제안한다. 이 새로운 KD 방법을 활용하여 대규모 "교사" BERT에 인코딩된 풍부한 지식을 작은 "학생" TinyBERT로 효과적으로 전달한다.

이어서 사전 학습 및 작업별 학습 단계 모두에서 트랜스포머 증류를 수행하는 새로운 2단계 학습 프레임워크를 도입한다. 이 프레임워크는 TinyBERT가 BERT의 일반 도메인 및 작업별 지식을 모두 포착할 수 있도록 보장한다.

실험 결과 4개 레이어로 구성된 TinyBERT4는 GLUE 벤치마크에서 교사 모델인 BERT BASE 성능의 96.8% 이상을 달성하며, 크기는 7.5배 작고 추론 속도는 9.4배 빠르다. 또한 TinyBERT4는 4개 레이어 BERT 증류의 최신 기준 모델보다 약 28% 적은 파라미터와 약 31%의 추론 시간으로 훨씬 더 우수하다. 6개 레이어로 구성된 TinyBERT6는 교사 모델인 BERT BASE$와 동등한 성능을 보인다.

# Introduction
![image](/images/TinyBERT review/TinyBERT review_3.png)![image](/images/TinyBERT review/TinyBERT review_5.png)![image](/images/TinyBERT review/TinyBERT review_6.png)![image](/images/TinyBERT review/TinyBERT review_7.png)

# Method
![image](/images/TinyBERT review/TinyBERT review_8.png)
![image](/images/TinyBERT review/TinyBERT review_9.png)![image](/images/TinyBERT review/TinyBERT review_10.png)![image](/images/TinyBERT review/TinyBERT review_11.png)![image](/images/TinyBERT review/TinyBERT review_12.png)![image](/images/TinyBERT review/TinyBERT review_13.png)![image](/images/TinyBERT review/TinyBERT review_14.png)![image](/images/TinyBERT review/TinyBERT review_15.png)![image](/images/TinyBERT review/TinyBERT review_16.png)![image](/images/TinyBERT review/TinyBERT review_17.png)![image](/images/TinyBERT review/TinyBERT review_18.png)![image](/images/TinyBERT review/TinyBERT review_19.png)![image](/images/TinyBERT review/TinyBERT review_20.png)![image](/images/TinyBERT review/TinyBERT review_21.png)

# Experiments
![image](/images/TinyBERT review/TinyBERT review_22.png)
![image](/images/TinyBERT review/TinyBERT review_23.png)![image](/images/TinyBERT review/TinyBERT review_24.png)![image](/images/TinyBERT review/TinyBERT review_25.png)![image](/images/TinyBERT review/TinyBERT review_26.png)![image](/images/TinyBERT review/TinyBERT review_27.png)

# Ablation Studies
![image](/images/TinyBERT review/TinyBERT review_28.png)![image](/images/TinyBERT review/TinyBERT review_29.png)
![image](/images/TinyBERT review/TinyBERT review_30.png)

# Conclusion and Review
* Transformer 기반 모델에 대한 distillation 방법 제시
    * Transformer-layer, embedding-layer, prediction-layer
* TinyBERT 학습의 two-stage framework 제안
    * Pre-training, task-specific
* BERT-base의 모델 사이즈를 줄이고, 추론 시간 역시 감소시킴
    * 성능은 비슷하게 유지하면서 edge devices에서 사용 가능할 수 있는 BERT 기반 모델 제공
* ALBERT처럼 모델 사이즈를 줄이는 다양한 접근 방법들이 있음
* BERT의 over-parameterization 문제점에 잘 접근하여 distillation을 적용한 논문