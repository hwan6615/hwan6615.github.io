---
layout: post
title: "23. Paper Review: Faithful Chain-of-Thought Reasoning@ijcnlp'2023"
# date: 2016-06-19 10:00:00 +0900
categories: review
# tags: [LSTM, Anomaly Detection, ICML, Deep Learning]
---
[Paper Link](https://aclanthology.org/2023.ijcnlp-main.20.pdf)

# Abstract
Chain-of-Thought (CoT) 프롬프트는 복잡한 추론 작업에서 언어 모델(LM)의 성능을 향상시키지만, 생성된 추론 과정이 모델이 답변에 도달하는 방식을 반드시 반영하지는 않는다.

이러한 문제에 대응하기 위해 Faithful CoT라는 추론 프레임워크가 제안된다. 이 프레임워크는 두 가지 단계로 구성된다.

1.  **번역 (Translation)**: 자연어 쿼리를 기호 추론 체인으로 변환한다. 이 단계는 언어 모델이 수행한다.
    
2.  **문제 해결 (Problem Solving)**: 추론 체인을 사용하여 답변을 도출한다. 이 단계는 deterministic solver가 수행한다.
    
이러한 방식은 추론 체인이 최종 답변에 대한 충실한 설명을 제공하도록 보장한다. Faithful CoT는 해석 가능성 외에도 경험적 성능을 향상시킨다. Math Word Problems (MWP), Planning, Multi-hop Question Answering (QA), Relational Inference 등 4가지 다양한 도메인의 10개 벤치마크 중 9개에서 표준 CoT보다 우수한 성능을 보인다. 특히, MWP에서 6.3%, Planning에서 3.4%, Multi-hop QA에서 5.5%, Relational Inference에서 21.4%의 상대적 정확도 향상을 달성한다. 또한, GPT-4 및 Codex와 함께 7개 데이터셋에서 새로운 SOTA few-shot performance를 기록하며 (6개 데이터셋에서 95.0% 이상의 정확도), faithfulness와 accuracy 사이의 강력한 시너지를 보여준다.

# Introduction
![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_2.png)![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_3.png)![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_4.png)

# Proposed Methods
![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_5.png)![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_6.png)![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_7.png)![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_8.png)![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_9.png)![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_10.png)![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_11.png)![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_12.png)![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_13.png)![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_14.png)![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_15.png)

# Experiments
![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_16.png)![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_17.png)![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_18.png)![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_19.png)![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_20.png)![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_21.png)![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_22.png)![image](/images/Faithful Chain-of-Thought Reasoning/Faithful Chain-of-Thought Reasoning_23.png)

# Conclusion and Review
* Faithful CoT를 제안
	* Complex reasoning을 Translation, Problem Solving으로 decompose
* 4개 타입의 complex reasoning problems에서 new SOTA 달성
	* 7 of the 10 datasets
	* Faithful explanation을 추가적으로 제공
* 모델이 가질 수 있는 bias를 external solver를 통하여 보완했다고 생각
	* Natural language -> code style -> interpreter -> output
* Dataset에 따라 reasoning chain을 다르게 구성해야 함
	* Dataset에 맞는 symbolic language 사용
	* 복잡할 수 있지만 어떻게 보면 적합한 SL을 선택하여 SOTA 달성


