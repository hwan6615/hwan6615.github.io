---
layout: post
title: "19. Paper Review: Meet in the Middle: A New Pre-training Paradigm@arXiv'2023"
# date: 2016-06-19 10:00:00 +0900
categories: review
# tags: [LSTM, Anomaly Detection, ICML, Deep Learning]
---
[Paper Link](https://arxiv.org/pdf/2303.07295)

# Abstract
대부분의 언어 모델(LM)은 이전 토큰에만 의존하는 자동 회귀 방식(좌->우)으로 학습하고 적용된다. 그러나 이러한 방식은 학습 시 전체 시퀀스 정보 활용이나 추론 시 양방향 문맥 활용의 잠재적 이점을 무시한다.

이 논문은 학습 데이터 효율성과 언어 모델의 인필링(infilling) 작업 능력을 동시에 향상시키는 새로운 사전 학습 패러다임을 제안한다. 이 패러다임의 주요 기법은 두 가지이다.

1.  **학습 목표**: 좌->우 언어 모델의 예측값을 동일한 데이터로 역순으로 학습된 우->좌 언어 모델의 예측값과 정렬시키는 학습 목표를 사용한다.
    
2.  **양방향 추론 절차**: 두 언어 모델이 중간에서 만날 수 있도록 하는 양방향 추론 절차를 사용한다.
    
프로그래밍 언어 모델과 자연어 모델 모두에 대한 광범위한 실험을 통해 이 사전 학습 패러다임의 효과를 입증했으며, 강력한 기준선들을 능가하는 성능을 보여준다.

# Introduction
![image](/images/Meet in the Middle/Meet in the Middle_2.png)
![image](/images/Meet in the Middle/Meet in the Middle_3.png)![image](/images/Meet in the Middle/Meet in the Middle_4.png)![image](/images/Meet in the Middle/Meet in the Middle_5.png)![image](/images/Meet in the Middle/Meet in the Middle_6.png)

# Proposed Methods
![image](/images/Meet in the Middle/Meet in the Middle_7.png)
![image](/images/Meet in the Middle/Meet in the Middle_8.png)![image](/images/Meet in the Middle/Meet in the Middle_9.png)![image](/images/Meet in the Middle/Meet in the Middle_10.png)![image](/images/Meet in the Middle/Meet in the Middle_11.png)![image](/images/Meet in the Middle/Meet in the Middle_12.png)![image](/images/Meet in the Middle/Meet in the Middle_13.png)![image](/images/Meet in the Middle/Meet in the Middle_14.png)![image](/images/Meet in the Middle/Meet in the Middle_15.png)![image](/images/Meet in the Middle/Meet in the Middle_16.png)![image](/images/Meet in the Middle/Meet in the Middle_17.png)![image](/images/Meet in the Middle/Meet in the Middle_18.png)![image](/images/Meet in the Middle/Meet in the Middle_19.png)![image](/images/Meet in the Middle/Meet in the Middle_20.png)![image](/images/Meet in the Middle/Meet in the Middle_21.png)![image](/images/Meet in the Middle/Meet in the Middle_22.png)![image](/images/Meet in the Middle/Meet in the Middle_23.png)![image](/images/Meet in the Middle/Meet in the Middle_24.png)![image](/images/Meet in the Middle/Meet in the Middle_25.png)![image](/images/Meet in the Middle/Meet in the Middle_26.png)![image](/images/Meet in the Middle/Meet in the Middle_27.png)![image](/images/Meet in the Middle/Meet in the Middle_28.png)![image](/images/Meet in the Middle/Meet in the Middle_29.png)![image](/images/Meet in the Middle/Meet in the Middle_30.png)

# Experiments
![image](/images/Meet in the Middle/Meet in the Middle_31.png)![image](/images/Meet in the Middle/Meet in the Middle_32.png)![image](/images/Meet in the Middle/Meet in the Middle_33.png)![image](/images/Meet in the Middle/Meet in the Middle_34.png)![image](/images/Meet in the Middle/Meet in the Middle_35.png)![image](/images/Meet in the Middle/Meet in the Middle_36.png)![image](/images/Meet in the Middle/Meet in the Middle_37.png)![image](/images/Meet in the Middle/Meet in the Middle_38.png)![image](/images/Meet in the Middle/Meet in the Middle_39.png)![image](/images/Meet in the Middle/Meet in the Middle_40.png)![image](/images/Meet in the Middle/Meet in the Middle_41.png)![image](/images/Meet in the Middle/Meet in the Middle_42.png)![image](/images/Meet in the Middle/Meet in the Middle_43.png)![image](/images/Meet in the Middle/Meet in the Middle_44.png)

# Conclusion and Review
* LLMs의 두 가지 challenges를 해결하고자 했음
	* Pre-training data efficiency
	* Infilling task에서 context를 더 잘 다루고자 함
* "Meet in the Middle"을 제안
	* Forward, backward LMs를 통하여 next token prediction 뿐만 아니라 일치하도록 학습
	* Pre-training의 결과로 forward LM을 기존 autoregressive LMs의 대체로 사용할 수 있음
* Infilling task에 적합한 inference procedure를 제안
* FIM 모델이 data-level에서 prefix, middle, suffix를 구성하여 학습을 시켰고 unnatural한 구성이지만 성능은 괜찮았음
	* (Prefix + suffix) -> middle 예측
	* 하지만 suboptimal함 -> 사람의 관점에서 생각했을 때 좋지 않은 구성
* MIM 모델은 data에 변형을 주지 않고 학습 단계에서 양방향 정보를 주입
	* Middle positions에 대한 sequence generation은 결국 양방향 context를 사용하는 것이 필요함
	* Data 변형, information leakage 없이 보다 깔끔하게 구성되었다고 생각
