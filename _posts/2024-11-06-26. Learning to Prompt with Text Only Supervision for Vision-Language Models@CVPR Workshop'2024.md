---
layout: post
title: "26. Paper Review: Learning to Prompt with Text Only Supervision for Vision-Language Models@CVPR Workshop'2024"
# date: 2016-06-19 10:00:00 +0900
categories: review
# tags: [LSTM, Anomaly Detection, ICML, Deep Learning]
---
[Paper Link](https://arxiv.org/pdf/2401.02418)

# Abstract
이 논문은 비전-언어 모델(Vision-Language Models, VLM)의 일반화 능력 향상을 목표로 한다. CLIP과 같은 VLM은 뛰어난 일반화 능력을 보이지만, 이를 다운스트림 작업에 적용하는 것은 여전히 어려운 과제이다. 기존 방식들은 주로 시각 정보를 사용하여 프롬프트를 학습하지만, 이는 레이블링된 데이터가 필요하고 , 원본 데이터에 과적합되어 새로운 데이터셋에 대한 일반화가 어렵다.

다른 접근 방식으로는 대규모 언어 모델(Large Language Models, LLMs)을 사용하여 클래스 설명을 생성하고 프롬프트 앙상블을 수행하는 훈련 없는(training-free) 방법이 있다. 하지만 이 방법들은 클래스별 프롬프트를 생성하여 다른 클래스로 전이할 수 없다는 한계가 있으며, 각 클래스에 대해 별도로 LLM 설명을 생성해야 하므로 비용이 많이 든다.

이러한 한계점을 극복하기 위해, 이 연구에서는 LLM에서 파생된 텍스트 데이터만을 사용하여 프롬프트를 학습하는 방법을 제안한다. 이미지가 없는 환경에서 프롬프트의 지도 학습이 쉽지 않기 때문에, LLM 데이터에서 풍부한 문맥적 지식을 추출할 수 있는 훈련 접근 방식을 개발한다. 학습된 프롬프트에 LLM 문맥 데이터가 매핑되어 새로운 클래스와 데이터셋으로 프롬프트의 제로샷 전이(zero-shot transfer)가 가능해져, LLM 프롬프트 엔지니어링 비용을 잠재적으로 절감할 수 있다. 본 논문에서는 텍스트 데이터만을 사용하여 일반화된 프롬프트를 학습하는 최초의 연구라고 주장한다. 4가지 벤치마크에 대한 광범위한 평가를 통해 기존 앙상블 방식을 능가하며, 레이블링된 이미지를 사용하는 방식과도 경쟁력 있는 성능을 보여준다.

# Introduction
![image](/images/ProText review/ProText review_3.png)
![image](/images/ProText review/ProText review_4.png)![image](/images/ProText review/ProText review_5.png)![image](/images/ProText review/ProText review_6.png)![image](/images/ProText review/ProText review_7.png)![image](/images/ProText review/ProText review_8.png)

# Proposed Methods
![image](/images/ProText review/ProText review_10.png)
![image](/images/ProText review/ProText review_11.png)![image](/images/ProText review/ProText review_12.png)![image](/images/ProText review/ProText review_13.png)![image](/images/ProText review/ProText review_14.png)![image](/images/ProText review/ProText review_15.png)![image](/images/ProText review/ProText review_16.png)![image](/images/ProText review/ProText review_17.png)

# Experiments
![image](/images/ProText review/ProText review_19.png)
![image](/images/ProText review/ProText review_20.png)![image](/images/ProText review/ProText review_21.png)![image](/images/ProText review/ProText review_22.png)![image](/images/ProText review/ProText review_23.png)![image](/images/ProText review/ProText review_24.png)![image](/images/ProText review/ProText review_25.png)![image](/images/ProText review/ProText review_26.png)![image](/images/ProText review/ProText review_27.png)
![image](/images/ProText review/ProText review_29.png)![image](/images/ProText review/ProText review_30.png)![image](/images/ProText review/ProText review_31.png)![image](/images/ProText review/ProText review_32.png)![image](/images/ProText review/ProText review_33.png)

# Conclusion and Review
* CLIP을 adapt할 수 있는 새로운 방법인 **ProText**를 제안
* 기존 방식들의 한계점을 극복
	* Prompt Learning : Text supervision 만을 사용
	* Prompt Ensembling : Prompt learning을 하여 transferability를 높임
* Transferability와 일반화에 큰 강점이 있는 방법
* Learnable prompt에 object(class)에 대한 자세한 정보가 내포되어있음
* 연구에 적용해볼 점은 salient object에 추가적인 정보(feature)로서 learnable prompt를 사용해볼 수 있음
