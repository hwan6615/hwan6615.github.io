---
layout: post
title: "3. Paper Review: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding@NAACL'2018"
# date: 2016-06-19 10:00:00 +0900
categories: review
# tags: [LSTM, Anomaly Detection, ICML, Deep Learning]
---
[Paper Link](https://aclanthology.org/N19-1423.pdf)

# Abstract
BERT(Bidirectional Encoder Representations from Transformers)는 새로운 언어 표현 모델이다. 이 모델은 기존의 언어 표현 모델들과 달리, 모든 레이어에서 왼쪽 및 오른쪽 문맥을 모두 고려하여 레이블이 지정되지 않은 텍스트로부터 깊은 양방향 표현을 사전 학습하도록 설계되었다. 
그 결과, 사전 학습된 BERT 모델은 추가적인 출력 레이어 하나만으로 질문 응답 및 언어 추론과 같은 광범위한 작업에서 최고 수준의 성능을 달성할 수 있으며, 작업별 아키텍처를 크게 수정할 필요가 없다. BERT는 개념적으로 간단하지만, GLUE 스코어를 80.5%(7.7% 포인트 향상), MultiNLI 정확도를 86.7%(4.6% 향상), SQUAD v1.1 질문 응답 Test F1을 93.2(1.5% 포인트 향상), SQUAD v2.0 Test F1을 83.1(5.1% 포인트 향상)로 끌어올리는 등 11가지 자연어 처리 작업에서 새로운 최고 성능을 달성하며 강력한 성능을 보여주었다. 

# Introduction
![image](/images/BERT paper review/BERT paper review_2.png)
![image](/images/BERT paper review/BERT paper review_3.png)![image](/images/BERT paper review/BERT paper review_4.png)![image](/images/BERT paper review/BERT paper review_5.png)![image](/images/BERT paper review/BERT paper review_6.png)![image](/images/BERT paper review/BERT paper review_7.png)
![image](/images/BERT paper review/BERT paper review_8.png)
![image](/images/BERT paper review/BERT paper review_9.png)![image](/images/BERT paper review/BERT paper review_10.png)

# Model Architecture
![image](/images/BERT paper review/BERT paper review_11.png)![image](/images/BERT paper review/BERT paper review_12.png)
![image](/images/BERT paper review/BERT paper review_13.png)

# Input/Output Representations
![image](/images/BERT paper review/BERT paper review_14.png)![image](/images/BERT paper review/BERT paper review_15.png)
![image](/images/BERT paper review/BERT paper review_16.png)
![image](/images/BERT paper review/BERT paper review_17.png)
![image](/images/BERT paper review/BERT paper review_18.png)
![image](/images/BERT paper review/BERT paper review_19.png)
![image](/images/BERT paper review/BERT paper review_20.png)

# Training
## Pre-training
![image](/images/BERT paper review/BERT paper review_21.png)
![image](/images/BERT paper review/BERT paper review_22.png)
![image](/images/BERT paper review/BERT paper review_23.png)![image](/images/BERT paper review/BERT paper review_24.png)![image](/images/BERT paper review/BERT paper review_25.png)![image](/images/BERT paper review/BERT paper review_26.png)
![image](/images/BERT paper review/BERT paper review_27.png)
## Fine-tuning
![image](/images/BERT paper review/BERT paper review_28.png)
##  Pre-training Procedure
![image](/images/BERT paper review/BERT paper review_29.png)![image](/images/BERT paper review/BERT paper review_30.png)![image](/images/BERT paper review/BERT paper review_31.png)
![image](/images/BERT paper review/BERT paper review_32.png)

# Dataset
![image](/images/BERT paper review/BERT paper review_33.png)![image](/images/BERT paper review/BERT paper review_34.png)![image](/images/BERT paper review/BERT paper review_35.png)![image](/images/BERT paper review/BERT paper review_36.png)![image](/images/BERT paper review/BERT paper review_37.png)![image](/images/BERT paper review/BERT paper review_38.png)![image](/images/BERT paper review/BERT paper review_39.png)![image](/images/BERT paper review/BERT paper review_40.png)![image](/images/BERT paper review/BERT paper review_41.png)![image](/images/BERT paper review/BERT paper review_42.png)

# Experiments
![image](/images/BERT paper review/BERT paper review_43.png)![image](/images/BERT paper review/BERT paper review_44.png)![image](/images/BERT paper review/BERT paper review_45.png)![image](/images/BERT paper review/BERT paper review_46.png)![image](/images/BERT paper review/BERT paper review_47.png)![image](/images/BERT paper review/BERT paper review_48.png)![image](/images/BERT paper review/BERT paper review_49.png)![image](/images/BERT paper review/BERT paper review_50.png)![image](/images/BERT paper review/BERT paper review_51.png)![image](/images/BERT paper review/BERT paper review_52.png)
# Ablation Studies
![image](/images/BERT paper review/BERT paper review_53.png)![image](/images/BERT paper review/BERT paper review_54.png)

# Conclusion and Review
* Feature-based 접근 방식은 task-specific하게 model architecture가 다름
* Fine-tuning 접근 방식을 통하여 하나의 모델로 여러가지 테스크에 적용할 수 있음
* BERT 이전 SOTA 모델보다 대부분의 테스크에서 더 좋은 성능을 보임(Deep Bidirectional Model을 사용)
* 하나의 모델로 여러 테스크에 적용하면 효과적일 것이라는 생각은 누구나 해볼 수 있지만, 실제로 구현했다는 것에서 굉장히 혁신적임
* 지금도 fine-tuning을 활용한 여러 변형된 모델들이 나오는 것으로 보아, 더 보편적으로 활용될 수 있는 모델이 나오는 데에는 오랜 시간이 걸릴 것으로 예상