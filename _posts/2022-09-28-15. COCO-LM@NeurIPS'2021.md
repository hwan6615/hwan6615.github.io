---
layout: post
title: "15. Paper Review: COCO-LM:Correcting and Contrasting Text Sequences for Language Model Pretraining@NeurIPS'2021"
# date: 2016-06-19 10:00:00 +0900
categories: review
# tags: [LSTM, Anomaly Detection, ICML, Deep Learning]
---
[Paper Link](https://arxiv.org/pdf/2102.08473)

# Abstract
COCO-LM은 손상된 텍스트 시퀀스를 교정하고 대조하여 언어 모델을 사전 훈련하는 자체 지도 학습 프레임워크이다. 이 프레임워크는 ELECTRA 스타일의 사전 훈련 방식을 따르며, 보조 언어 모델을 사용하여 텍스트 시퀀스를 손상시키고, 주 모델 사전 훈련을 위한 두 가지 새로운 작업을 구축한다.

첫 번째는 토큰 수준의 작업인 'Corrective Language Modeling (CLM)'으로, 보조 모델에 의해 대체된 토큰을 탐지하고 수정하여 토큰 수준의 의미를 더 잘 포착하도록 한다. 두 번째는 시퀀스 수준의 작업인 'Sequence Contrastive Learning (SCL)'로, 동일한 원본 입력에서 파생된 텍스트 시퀀스를 정렬하고 표현 공간의 균일성을 확보한다.

GLUE 및 SQUAD 벤치마크에 대한 실험 결과, COCO-LM은 SOTA 사전 훈련 모델보다 정확도 면에서 우수할 뿐만 아니라 사전 훈련 효율성도 향상시킴을 보여준다. COCO-LM은 ELECTRA의 MNLI 정확도를 50%의 사전 훈련 GPU 시간으로 달성한다. 표준 기본/대형 모델과 동일한 사전 훈련 단계를 사용하여 COCO-LM은 이전 최고 모델보다 GLUE 평균 점수에서 1점 이상 향상된 성능을 보인다.

# Introduction
![image](/images/COCO-LM/COCO-LM_2.png)
![image](/images/COCO-LM/COCO-LM_3.png)![image](/images/COCO-LM/COCO-LM_4.png)![image](/images/COCO-LM/COCO-LM_5.png)![image](/images/COCO-LM/COCO-LM_6.png)![image](/images/COCO-LM/COCO-LM_7.png)

# Proposed Method
![image](/images/COCO-LM/COCO-LM_8.png)
![image](/images/COCO-LM/COCO-LM_9.png)![image](/images/COCO-LM/COCO-LM_10.png)![image](/images/COCO-LM/COCO-LM_11.png)![image](/images/COCO-LM/COCO-LM_12.png)![image](/images/COCO-LM/COCO-LM_13.png)![image](/images/COCO-LM/COCO-LM_14.png)![image](/images/COCO-LM/COCO-LM_15.png)![image](/images/COCO-LM/COCO-LM_16.png)![image](/images/COCO-LM/COCO-LM_17.png)![image](/images/COCO-LM/COCO-LM_18.png)![image](/images/COCO-LM/COCO-LM_19.png)

# Experiment
![image](/images/COCO-LM/COCO-LM_20.png)
![image](/images/COCO-LM/COCO-LM_21.png)![image](/images/COCO-LM/COCO-LM_22.png)![image](/images/COCO-LM/COCO-LM_23.png)![image](/images/COCO-LM/COCO-LM_24.png)![image](/images/COCO-LM/COCO-LM_25.png)![image](/images/COCO-LM/COCO-LM_26.png)![image](/images/COCO-LM/COCO-LM_27.png)

# Conclusion and Review
* ELECTRA의 efficiency를 채택하되, 추가적인 pre-training 방법론을 제시
	* **C**orrective **L**anguage **M**odeling
	* **S**equence **C**ontrastive **L**earning
* 이를 통하여 efficiency와 anisotropy 문제를 완화시킴
* GLUE, SQuAD benchmarks에서 기존 SOTA보다 높은 성능을 보임
* Pre-training computing resources와 network parameters의 관점에서 efficient함
* Contrastive learning을 pre-training 관점에서 적용했지만 dramatic한 성능 향상을 보인 것은 아님
* STS-B task와 같이 두 문장 간의 연관성을 파악하는 관점에서 fine-tuning에서도 contrastive learning을 진행한다면 더 성능이 오를 것 같음(like SimCSE)
* Sequence representations에서 나타나는 문제(anisotropy)의 완화를 가시적으로 확인하니 contrastive learning이 확실히 도움이 된다는 생각
