---
layout: post
title: "25. Paper Review: DECAP: DECODING CLIP LATENTS FOR ZERO-SHOT CAPTIONING VIA TEXT-ONLY TRAINING@ICLR’2023"
# date: 2016-06-19 10:00:00 +0900
categories: review
# tags: [LSTM, Anomaly Detection, ICML, Deep Learning]
---
[Paper Link](https://arxiv.org/pdf/2303.03032)

# Abstract
CLIP과 같은 대규모 사전 학습된 멀티모달 모델은 이미지 분류와 같은 판별 작업에서 강력한 제로샷 전이 능력을 보여준다. 그러나 기존의 제로샷 캡셔닝 방법들은 대규모 언어 모델(예: GPT-2)을 활용하거나 인코더-디코더 네트워크를 엔드투엔드로 사전 학습시키는 방식이다. 대규모 언어 모델은 캡셔닝과의 작업 불일치로 인해 적절한 설명을 생성하지 못할 수 있고, 엔드투엔드 사전 학습은 쌍을 이루는 데이터와 방대한 계산 자원을 필요로 한다.
    
이러한 한계를 해결하기 위해, 이 논문은 DeCap이라는 간단한 제로샷 캡셔닝 프레임워크를 제안한다. DeCap은 경량의 시각 인식 언어 디코더를 도입하며, 이 디코더는 데이터 효율적이고 계산 효율적이다. 이는 훈련에 텍스트 데이터만 필요하며, 엔드투엔드 훈련이 필요하지 않다.
    
디코더는 텍스트 전용 데이터로 훈련되며, CLIP 인코더에서 추출된 텍스트 임베딩을 prefix 임베딩으로 사용한다. 추론 단계에서는 시각 입력에 기반하여 캡션을 생성해야 하지만, CLIP 텍스트 임베딩과 시각 임베딩 사이의 modality gap이 존재하여 시각 임베딩을 직접 prefix 임베딩으로 사용할 수 없다.
    
이를 해결하기 위해 훈련 없이 modality gap을 줄이는 메커니즘을 제안한다. 이 메커니즘은 시각 임베딩을 CLIP 텍스트 임베딩 공간으로 투영하며, 투영된 임베딩은 시각 입력의 정보를 유지한다. 투영된 임베딩을 prefix 임베딩으로 사용하여 디코더는 시각 입력과 일치하는 고품질 설명을 생성한다. 실험 결과, DeCap은 MSCOCO 및 NoCaps와 같은 일반적인 이미지 캡셔닝 벤치마크에서 다른 제로샷 캡셔닝 방법 및 unpaired 캡셔닝 방법보다 우수한 성능을 보인다. 또한 DeCap은 비디오 캡셔닝에도 적용되어 MSR-VTT 및 ActivityNet-Captions에서 최첨단 제로샷 성능을 달성한다.

# Introduction
![image](/images/DECAP review/DECAP review_3.png)
![image](/images/DECAP review/DECAP review_4.png)![image](/images/DECAP review/DECAP review_5.png)![image](/images/DECAP review/DECAP review_6.png)![image](/images/DECAP review/DECAP review_7.png)![image](/images/DECAP review/DECAP review_8.png)![image](/images/DECAP review/DECAP review_9.png)![image](/images/DECAP review/DECAP review_10.png)

# Proposed Methods
![image](/images/DECAP review/DECAP review_12.png)
![image](/images/DECAP review/DECAP review_13.png)![image](/images/DECAP review/DECAP review_14.png)![image](/images/DECAP review/DECAP review_15.png)![image](/images/DECAP review/DECAP review_16.png)![image](/images/DECAP review/DECAP review_17.png)![image](/images/DECAP review/DECAP review_18.png)![image](/images/DECAP review/DECAP review_19.png)![image](/images/DECAP review/DECAP review_20.png)

# Experiments
![image](/images/DECAP review/DECAP review_22.png)
![image](/images/DECAP review/DECAP review_23.png)![image](/images/DECAP review/DECAP review_24.png)![image](/images/DECAP review/DECAP review_25.png)![image](/images/DECAP review/DECAP review_26.png)![image](/images/DECAP review/DECAP review_27.png)![image](/images/DECAP review/DECAP review_28.png)![image](/images/DECAP review/DECAP review_29.png)
![image](/images/DECAP review/DECAP review_31.png)
![image](/images/DECAP review/DECAP review_32.png)![image](/images/DECAP review/DECAP review_33.png)

# Conclusion and Review
* Zero-shot captioning framework **DeCap**을 제안
	* Text-only data를 활용하여 lightweight visual-aware languare decoder를 학습
	* Training-free mechanism
		* Visual embedding을 text embedding space로 projection
		* Modality gap을 줄임
	* Decoder와 projection mechanism을 결합하여 SOTA 달성
* Zero-shot captioning에서는 CLIP을 활용하는 것이 당연시 됨
* 단순히 CLIP embedding을 decoder의 input으로 할용하는 것이 아닌 modality gap을 줄이기 위한 방식을 제안
* Decoder 단에서 prefix pre-training을 진행하여 captioning task에 적합한 모델을 만든 것이 가장 큰 contribution이라고 생각
* 하지만 support memory에 구성된 text에 의존성이 큼
