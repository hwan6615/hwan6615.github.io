---
layout: post
# title: "13. Paper Review: QRelScore: Better Evaluating Generated Questions with Deeper Understanding of Context-aware Relevance@EMNLP'2022"
# date: 2016-06-19 10:00:00 +0900
categories: Documentation
tags: [Deep Learning]
---
# Introduction
![image](/images/Lecture-8 Second Order Optimization/intro1.png)

# Contents
![image](/images/Lecture-8 Second Order Optimization/content1.png)
![image](/images/Lecture-8 Second Order Optimization/Lecture-8 Second Order Optimization2.png)
![image](/images/Lecture-8 Second Order Optimization/Lecture-8 Second Order Optimization3.png)
![image](/images/Lecture-8 Second Order Optimization/Lecture-8 Second Order Optimization4.png)
![image](/images/Lecture-8 Second Order Optimization/content2.png)

# Conclusion
Second-Order Optimization은 First-Order Optimization보다  더  많은  정보를  활용하여 Loss가 0인  곳으로  더  잘  도달할  가능성이  있지만, 곧장 minima로  가는  것이  아니라 minima의  방향으로  가는  것이고, 결국에는 Learning Rate을  사용하지  않기  때문에  좋지  않은 Optimization이다.

# Reference
[1]  Stanford University Course cs231n, “CS231n Convolutional Neural Networks for Visual Recognition,” _Stanford Univ. Course cs231n_, 2018.

[2] 박주찬, “second order optimization,” 2020. https://www.notion.so/3e3a3d25f5f74149a80d6580c9425983?v=a3468d54b4df468ea93a30875db66283.

[3] 존이, “[수치해석]4.테일러  급수,” 2014, [Online]. Available: https://blog.naver.com/mykepzzang/220067265852.
