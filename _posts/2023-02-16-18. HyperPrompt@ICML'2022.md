---
layout: post
title: "18. Paper Review: HyperPrompt: Prompt-based Task-Conditioning of Transformersg@ICML'2022"
# date: 2016-06-19 10:00:00 +0900
categories: review
# tags: [LSTM, Anomaly Detection, ICML, Deep Learning]
---
[Paper Link](https://arxiv.org/pdf/2203.00759)

# Abstract
이 논문은 사전 학습된 언어 모델을 파라미터 효율적인 방식으로 미세 조정하는 새로운 패러다임인 Prompt-Tuning의 확장을 다룬다. 논문은 HyperNetworks를 활용하여 트랜스포머의 self-attention에 프롬프트 기반 태스크 컨디셔닝을 위한 새로운 아키텍처인 HyperPrompt를 제안한다. HyperPrompt는 HyperNetwork를 통해 종단 간 학습이 가능하며, 쿼리가 참조할 수 있는 태스크 전역 메모리 역할을 하면서도 태스크 간 유연한 정보 공유를 가능하게 한다. HyperPrompt는 적은 수의 추가 태스크 컨디셔닝 파라미터(0.14% 수준)만으로도 강력한 멀티태스크 학습 기준선에 필적하는 성능을 보여주며, 뛰어난 파라미터 및 연산 효율성을 달성한다. 광범위한 실험을 통해 HyperPrompt가 다양한 모델 크기에 걸쳐 GLUE 및 SuperGLUE 자연어 이해 벤치마크에서 Prompt-Tuning 및 HyperFormer++를 포함한 강력한 T5 멀티태스크 학습 baselines 및 파라미터 효율적인 어댑터 변형보다 우수한 성능을 달성함을 입증한다.

# Introduction
![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_2.png)
![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_3.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_4.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_5.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_6.png)

# Proposed Methods
![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_7.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_10.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_11.png)
![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_12.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_13.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_14.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_15.png)
![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_17.png)
![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_19.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_20.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_21.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_22.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_23.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_24.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_25.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_26.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_27.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_28.png)

# Experiments
![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_29.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_30.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_31.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_32.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_33.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_34.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_35.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_36.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_37.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_38.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_39.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_40.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_41.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_42.png)![image](images/HyperPrompt/HyperPrompt Prompt-based Task-Conditioning of Transformers_43.png)

# Conclusion and Review
* Novel architecture HyperPrompt 제안
	* Transformer의 self-attention에 prompt-based task-conditioning
	* HyperNetwork를 통하여 hyper-prompts 생성
		* Tasks간 정보 공유와 동시에 parameters and computation efficient
		* Hyper-prompts는 task global memories의 역할
	* Task-specific feature maps를 학습
	* 기존 parameter efficient 모델들과 multi-task baselines 대비 SOTA 달성
* 기존 parameter efficient 방법은 대부분 single task로의 접근이지만 multi-task로 확장하면서 정보 공유를 효율적으로 하는 방식을 제안
* 결국 고난이도의 tasks를 수행하려면 모든 parameters를 학습하는 것이 필요 -> Adapter, prompt 개념의 확장
	* Adaptive module을 추가해서 성능을 높이려면 full finetuning을 해야함
