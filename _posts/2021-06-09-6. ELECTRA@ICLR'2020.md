---
layout: post
title: "6. Paper Review: ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS@ICLR’2020"
# date: 2016-06-19 10:00:00 +0900
categories: review
# tags: [LSTM, Anomaly Detection, ICML, Deep Learning]
---
[Paper Link](https://arxiv.org/pdf/2003.10555)

# Abstract
ELECTRA는 텍스트 인코더를 생성자 대신 판별자로 사전 학습하는 새로운 방법을 제안한다. 기존 BERT와 같은 Masked Language Modeling (MLM) 방식은 입력 토큰의 일부를 [MASK]로 대체하고 원본 토큰을 재구성하도록 모델을 훈련한다. 이러한 방법은 자연어 처리(NLP) 다운스트림 태스크에서 좋은 결과를 내지만, 효과적이기 위해서는 많은 계산 자원이 필요하다. 

ELECTRA는 '교체된 토큰 감지(replaced token detection)'라는 더욱 샘플 효율적인 사전 학습 태스크를 제안한다. 이 방식은 입력을 마스킹하는 대신, 작은 생성자 네트워크에서 샘플링된 그럴듯한 대체 토큰으로 일부 토큰을 교체하여 입력을 변형한다. 그런 다음, 변형된 토큰의 원래 신원을 예측하는 모델을 훈련하는 대신, 변형된 입력의 각 토큰이 생성자 샘플에 의해 교체되었는지 여부를 예측하는 판별 모델을 훈련한다. 

이 새로운 사전 학습 태스크는 마스킹된 작은 하위 집합 토큰에만 적용되는 것이 아니라 모든 입력 토큰에 대해 정의되기 때문에 MLM보다 효율적이다. 그 결과, ELECTRA는 동일한 모델 크기, 데이터 및 계산 자원이 주어졌을 때 BERT보다 훨씬 뛰어난 문맥 표현을 학습한다. 특히 작은 모델에서 이점이 두드러지는데, 예를 들어 ELECTRA는 4일 동안 1개의 GPU로 훈련하여 GPT(30배 더 많은 계산 자원으로 훈련됨)를 GLUE 자연어 이해 벤치마크에서 능가하는 성능을 보인다. 또한 ELECTRA는 대규모에서도 잘 작동하여 RoBERTa 및 XLNet과 비슷한 성능을 보이면서 계산 자원은 1/4 미만을 사용하고, 동일한 양의 계산 자원을 사용하면 이들보다 더 나은 성능을 보인다. 

핵심적으로, ELECTRA의 목표는 텍스트 인코더가 원본 입력 토큰과 작은 생성기 네트워크에 의해 생성된 고품질의 가짜 샘플을 구별하도록 훈련하는 것이다. 이는 마스킹된 언어 모델링에 비해 계산 효율성이 높고 다운스트림 태스크에서 더 나은 성능을 제공한다. 이 방법은 비교적 적은 계산 자원으로도 좋은 결과를 얻을 수 있어, 컴퓨팅 자원에 대한 접근성이 낮은 연구자와 실무자에게 사전 학습된 텍스트 인코더의 개발 및 적용을 더 용이하게 할 것으로 기대된다. 

# Introduction
![image](/images/ELECTRA review/ELECTRA review_2.png)
![image](/images/ELECTRA review/ELECTRA review_3.png)
![image](/images/ELECTRA review/ELECTRA review_4.png)
![image](/images/ELECTRA review/ELECTRA review_5.png)
![image](/images/ELECTRA review/ELECTRA review_6.png)
![image](/images/ELECTRA review/ELECTRA review_7.png)
![image](/images/ELECTRA review/ELECTRA review_8.png)

# Method
![image](/images/ELECTRA review/ELECTRA review_9.png)
![image](/images/ELECTRA review/ELECTRA review_10.png)![image](/images/ELECTRA review/ELECTRA review_11.png)![image](/images/ELECTRA review/ELECTRA review_12.png)![image](/images/ELECTRA review/ELECTRA review_13.png)![image](/images/ELECTRA review/ELECTRA review_14.png)![image](/images/ELECTRA review/ELECTRA review_15.png)![image](/images/ELECTRA review/ELECTRA review_16.png)

# Experiments
![image](/images/ELECTRA review/ELECTRA review_17.png)![image](/images/ELECTRA review/ELECTRA review_18.png)![image](/images/ELECTRA review/ELECTRA review_19.png)![image](/images/ELECTRA review/ELECTRA review_20.png)![image](/images/ELECTRA review/ELECTRA review_21.png)![image](/images/ELECTRA review/ELECTRA review_22.png)![image](/images/ELECTRA review/ELECTRA review_23.png)![image](/images/ELECTRA review/ELECTRA review_24.png)![image](/images/ELECTRA review/ELECTRA review_25.png)![image](/images/ELECTRA review/ELECTRA review_26.png)![image](/images/ELECTRA review/ELECTRA review_27.png)![image](/images/ELECTRA review/ELECTRA review_28.png)![image](/images/ELECTRA review/ELECTRA review_29.png)![image](/images/ELECTRA review/ELECTRA review_30.png)![image](/images/ELECTRA review/ELECTRA review_31.png)
![image](/images/ELECTRA review/ELECTRA review_32.png)

# Conclusion and Review
* 입력 시퀀스 정보를 전부 활용하는 것이 더 효과적
* 기존 MLM 방식을 사용하는 모델들보다 가볍게 사용할 수 있음
* 단순히 모델의 크기를 키우는 것이 중요한 것이 아닌, 얼마나 효과적으로 구조를 설계할 것인지를 생각하는 것이 중요함








