---
layout: post
title: "7. Paper Review: ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS@ ICLR’2020"
# date: 2016-06-19 10:00:00 +0900
categories: review
# tags: [LSTM, Anomaly Detection, ICML, Deep Learning]
---

[Paper Link](https://arxiv.org/pdf/1909.11942)

# Abstract
BERT 모델의 크기가 커질수록 다운스트림 태스크에서 성능이 향상되지만, GPU/TPU 메모리 한계와 학습 시간 증가로 인해 더 이상의 확장은 어려워진다. 이러한 문제를 해결하기 위해 ALBERT는 BERT의 메모리 소비를 줄이고 학습 속도를 높이는 두 가지 파라미터 감소 기법을 제안한다.

제안하는 방법론은 다음과 같다.

* 분해된 임베딩 파라미터화(Factorized embedding parameterization): 이 기법은 큰 단어장 임베딩 행렬을 두 개의 작은 행렬로 분해하여, 은닉층의 크기와 단어 임베딩의 크기를 분리한다. 이를 통해 단어 임베딩의 파라미터 크기를 크게 늘리지 않고도 은닉 크기를 확장할 수 있다.

* 교차 계층 파라미터 공유(Cross-layer parameter sharing): 이 기법은 파라미터가 네트워크의 깊이에 따라 증가하는 것을 방지한다. 기본적으로 모든 계층에서 파라미터를 공유하며, 이는 파라미터 효율성을 크게 향상시킨다.

* 문장 순서 예측(Sentence Order Prediction, SOP) 손실: ALBERT는 문장 간 응집력 모델링에 중점을 둔 자체 지도 학습 손실인 SOP를 도입한다. SOP는 기존 BERT의 다음 문장 예측(NSP) 손실의 비효율성을 해결하도록 설계되었다. SOP는 일관되게 다중 문장 입력이 있는 다운스트림 태스크에 도움이 되는 것으로 나타났다.

결과적으로 ALBERT는 BERT-large에 비해 파라미터 수가 적음에도 불구하고 GLUE, RACE, SQUAD 벤치마크에서 새로운 최첨단 결과를 달성한다. 예를 들어, ALBERT-large는 BERT-large보다 파라미터 수가 18배 적으며 학습 속도는 1.7배 빠르다. ALBERT-xxlarge는 BERT-large의 약 70% 파라미터로 SQUAD v1.1에서 1.9%, SQUAD v2.0에서 3.1%, MNLI에서 1.4%, SST-2에서 2.2%, RACE에서 8.4%의 성능 향상을 보인다. 또한, 동일한 학습 시간으로 비교했을 때 ALBERT-xxlarge는 BERT-large보다 평균적으로 1.5% 더 우수하며, RACE에서는 5.2%의 큰 차이를 보인다.


# Introduction
![image](/images/ALBERT review/ALBERT review_2.png)![image](/images/ALBERT review/ALBERT review_3.png)![image](/images/ALBERT review/ALBERT review_4.png)![image](/images/ALBERT review/ALBERT review_5.png)

# Method
![image](/images/ALBERT review/ALBERT review_6.png)![image](/images/ALBERT review/ALBERT review_7.png)![image](/images/ALBERT review/ALBERT review_8.png)![image](/images/ALBERT review/ALBERT review_9.png)![image](/images/ALBERT review/ALBERT review_10.png)![image](/images/ALBERT review/ALBERT review_11.png)![image](/images/ALBERT review/ALBERT review_12.png)![image](/images/ALBERT review/ALBERT review_13.png)![image](/images/ALBERT review/ALBERT review_14.png)![image](/images/ALBERT review/ALBERT review_15.png)

# Experiment
![image](/images/ALBERT review/ALBERT review_16.png)![image](/images/ALBERT review/ALBERT review_17.png)![image](/images/ALBERT review/ALBERT review_18.png)![image](/images/ALBERT review/ALBERT review_19.png)![image](/images/ALBERT review/ALBERT review_20.png)![image](/images/ALBERT review/ALBERT review_21.png)![image](/images/ALBERT review/ALBERT review_22.png)![image](/images/ALBERT review/ALBERT review_23.png)![image](/images/ALBERT review/ALBERT review_24.png)

# Conclusion and Review
* BERT와 같은 사전학습 모델들은 모델 사이즈를 증가시킴에 따라 성능 향상을 가져왔었음
    * 하지만 메모리 한계와 느린 학습속도 문제 발생
    * 이를 해결하기 위해 파라미터 수를 줄이는 기법 제안
* NSP의 단순한 학습방법의 문제점 제기
    * Topic prediction이 아닌 문장 사이의 내부 연관성을 파악하는 SOP 학습 방법 사용
* 기존 모델들에 비하여 성능 향상 및 학습속도 개선
* 하드웨어적 한계점을 인식하고 이를 개선하기 위한 접근을 했다는 점에서 일반적인 연구자의 입장을 생각해준게 느껴짐
* 어떻게 하면 파라미터 수를 줄이면서 성능도 같이 개선할 것인지 고민을 많이 한 것으로 보임
* 기존 모델들에서 좋다고 하는 기법들(학습 방법 등)에 대하여 의문을 계속 갖고 개선점은 무엇이 있을지 생각하는 것이 중요