---
layout: post
title: "28. Paper Review: Image Captioning with Multi-Context Synthetic Data@AAAI'2024"
# date: 2016-06-19 10:00:00 +0900
categories: review
# tags: [LSTM, Anomaly Detection, ICML, Deep Learning]
---
[Paper Link](https://arxiv.org/pdf/2305.18072)

# Abstract
이미지 캡셔닝은 수많은 주석이 달린 이미지-텍스트 쌍을 필요로 하며, 이는 막대한 주석 비용을 초래한다. 확산 모델 및 대규모 언어 모델과 같은 대규모 모델은 고품질 이미지 및 텍스트 생성에 탁월한 성능을 보인다. 이러한 잠재력은 캡셔닝 모델 학습을 위한 합성 이미지-텍스트 쌍을 생성하는 데 활용될 수 있다. 합성 데이터는 데이터 수집의 비용 및 시간 효율성을 향상하고, 특정 도메인에 대한 사용자 정의를 허용하며, 제로샷 성능을 위한 일반화 기능을 부트스트랩하고, 실제 데이터와 관련된 개인 정보 보호 문제를 피할 수 있다. 

그러나 기존 방법들은 합성 데이터만으로는 만족스러운 성능을 달성하는 데 어려움을 겪는다. 본 논문에서는 단순한 설명으로 생성된 이미지가 제한된 맥락으로 단일한 관점만 포착하여 실제 이미지에 널리 퍼져 있는 복잡한 장면과 일치하지 않는다는 문제점을 발견한다. 이를 해결하기 위해 다중 컨텍스트 데이터 생성을 도입하는 혁신적인 파이프라인을 제시한다. 초기 텍스트 코퍼스부터 시작하여, 이 접근 방식은 대규모 언어 모델을 사용하여 다양한 관점에서 동일한 장면을 묘사하는 여러 문장을 추출한다. 이러한 문장들은 여러 컨텍스트를 가진 단일 문장으로 요약한다. 그 후, 요약된 캡션을 통해 확산 모델을 사용하여 복잡한 이미지를 생성한다. 이 모델은 이 과정을 통해 제작된 합성 이미지-텍스트 쌍으로만 학습된다. 이 파이프라인의 효과는 도메인 내 및 교차 도메인 설정 모두에서 실험 결과로 검증되었으며, MSCOCO, Flickr30k 및 NoCaps와 같은 잘 알려진 데이터 세트에서 SOTA 성능을 달성한다.

# Introduction
![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_3.png)
![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_4.png)![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_5.png)![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_6.png)

# Proposed Methods
![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_8.png)
![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_9.png)![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_10.png)![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_11.png)![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_12.png)![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_13.png)![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_14.png)![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_15.png)

# Experiments
![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_17.png)
![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_18.png)![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_19.png)![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_20.png)![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_21.png)![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_22.png)![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_23.png)![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_24.png)![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_25.png)![image](/images/Image Captioning with Multi-Context Synthetic data/Image Captioning with Multi-Context Synthetic data@AAAI'2024_26.png)

# Conclusion and Review
* 단일 caption에서 생성된 synthetic image가 real-world image가 가진 multi-context의 특성과 대비됨을 발견
* ICSD 파이프라인을 제안
	* LLM, diffusion model을 결합하여 multi-context data 생성
* 생성된 데이터만을 활용하여 좋은 성능 달성
* IC에서 synthetic data만을 사용하여 SOTA 성능을 보인 것이 인상적임
* Uni-context와 multi-context image간의 차이를 명확히 식별
* SS1M같이 대규모 데이터에 대하여 text와 image를 생성하는 것에 대한 시간적 효율성에 대한 분석이 부족함
* 실제 생성된 multi-context caption, image 예시가 없음

