---
layout: post
title: "22. Paper Review: Active Retrieval Augmented Generation@EMNLP'2023"
# date: 2016-06-19 10:00:00 +0900
categories: review
# tags: [LSTM, Anomaly Detection, ICML, Deep Learning]
---
[Paper Link](https://aclanthology.org/2023.emnlp-main.495.pdf)

# Abstract
이 논문은 대규모 언어 모델(LM)이 사실과 다른 내용을 생성하는 경향이 있음을 지적하며, 외부 지식 자원에서 정보를 검색하여 LM을 보강하는 방법을 제안한다. 기존의 검색 증강 LM은 입력에 기반하여 한 번만 정보를 검색하는 방식이었다. 그러나 이는 긴 텍스트 생성과 같은 일반적인 시나리오에서는 생성 과정 내내 지속적으로 정보를 수집하는 것이 중요하다는 점에서 한계가 있다.

이 논문은 생성 과정에서 언제, 무엇을 검색할지 능동적으로 결정하는 '능동 검색 증강 생성'에 대한 일반화된 관점을 제공한다. 다가올 문장의 예측을 반복적으로 사용하여 미래 내용을 예상하고, 이를 쿼리로 활용하여 낮은 신뢰도의 토큰이 포함된 문장을 다시 생성하기 위해 관련 문서를 검색하는 "**F**orward-**L**ooking **A**ctive **RE**trieval augmented generation(**FLARE**)"라는 일반적인 방법을 제안한다. 4가지 장문 지식 집중 생성 작업/데이터셋에 걸쳐 FLARE와 baseline 모델들을 포괄적으로 테스트하였다. FLARE는 모든 작업에서 우수하거나 경쟁력 있는 성능을 달성하여 제안된 방법의 효과를 입증하였다.

# Introduction
![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_2.png)
![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_3.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_4.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_5.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_6.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_7.png)

# Proposed Methods
![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_8.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_9.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_10.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_11.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_12.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_13.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_14.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_15.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_16.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_17.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_18.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_19.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_20.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_21.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_22.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_23.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_24.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_25.png)

# Experiments
![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_26.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_27.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_28.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_29.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_30.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_31.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_32.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_33.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_34.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_35.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_36.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_37.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_38.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_39.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_40.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_41.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_42.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_43.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_44.png)![image](/images/Active Retrieval Augmented Generation/Active Retrieval Augmented Generation_45.png)

# Conclusion and Review
* Active retrieval augmented generation framework를 제안
	* 생성 과정에서 retrieval augmentation으로 long-form generation에 도움
	* When, what to retrieve를 active하게 결정
* 해당 framework를 forward-looking active retrieval과 함께 사용
	* Low-confidence tokens를 가지는 upcoming sentence를 활용하여 relevant information을 retrieve
* 4개의 tasks/datasets에서 효과를 증명
* Retrieval augmented generation의 일반화를 위해 고려한 요소들이 framework에 반영이 잘 되었다고 생각
	* When, What to retrieve -> Query selection, 임의의 sentence 생성 후 retrieve 할 것인지를 판단 등
	* 확실히 호흡이 긴 long-form task의 경우 next sentence를 활용한 retrieval이 도움이 되었음
		* Generation step이 지나면 지날수록 과거의 정보가 상대적으로 덜 중요해지는 경향성이 있기 때문
	* 따라서 short-form tasks에 대한 성능도 있었으면 한다는 아쉬움
	* 이러한 점에서 과연 일반화가 잘 된 framework 인가? 조금의 의문점
