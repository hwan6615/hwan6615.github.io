---
layout: post
title: "10. Paper Review: Exploring the Role of BERT Token Representation to Explain Sentence Probing Results@EMNLP'2021"
# date: 2016-06-19 10:00:00 +0900
categories: review
# tags: [LSTM, Anomaly Detection, ICML, Deep Learning]
---
[Paper Link](https://aclanthology.org/2021.emnlp-main.61.pdf)

# Abstract
BERT(Bidirectional Encoder Representations from Transformers)의 토큰 표현이 문장 탐색 결과(sentence probing results)를 설명하는 역할을 탐구하며 주요 내용은 아래와 같다.

* BERT는 특정 토큰 표현에 의미 있는 지식을 인코딩하는 경향이 있으며, 이는 종종 표준 분류 설정에서 무시된다.
* 이러한 토큰 표현은 모델이 구문 및 의미적 이상 징후를 감지하고 문법적 수와 시제 부분 공간을 명확하게 분리하는 데 기여한다.
* 연구는 탐색 작업을 통해 BERT의 표현 공간에서 명확하고 의미 있는 부분 공간을 찾기 위한 심층 분석을 제공한다.

# Introduction
![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_2.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_3.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_4.jpg)
![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_6.jpg)
# Methods
![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_7.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_8.jpg)
![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_9.jpg)
![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_10.jpg)
# Experiments
![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_11.jpg)
![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_12.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_13.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_14.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_15.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_16.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_17.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_18.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_19.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_20.jpg)
![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_21.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_22.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_23.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_24.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_25.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_26.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_27.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_28.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_29.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_30.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_31.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_32.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_33.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_34.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_35.jpg)![image](/images/Exploring the Role of BERT TR review/Exploring the Role of BERT Token Representations review_36.jpg)

# Conclusion and Review
* BERT의 token representations를 gradient-based attribution을 통하여 분석한 논문
    * Probing performance를 token representations 관점에서 설명한 첫 번째 시도
* Layer마다 성능에 영향을 주는 token representations를 여러 probing tasks(surface-, syntatic-, semantic-level)를 통하여 분석
* Fine-tuning 시 subspaces가 어떻게 변하는 지와 해당 부분이 inference 시 CoLA와 같은 grammar-based tasks에서 어떻게 유효하게 작용하는지를 future work로 두었음
* 사전학습 언어모델에 대한 분석 논문을 읽은 것은 처음이었음
* 단순히 low layers에서 syntatic한 정보를 high layers에서는 semantic한 정보를 잡는 것에서 나아가 token-level에서 분석한 것이 신선함
* Probing tasks마다 의도했던 token representations에서 saliency score가 높은 것을 통해 사전학습 언어 모델이 어느정도 사람처럼 학습되었다고 생각해볼 수 있음