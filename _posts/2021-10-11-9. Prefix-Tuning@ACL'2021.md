---
layout: post
title: "9. Paper Review: Prefix-Tuning: Optimizing Continuous Prompts for Generation@ACL'2021"
# date: 2016-06-19 10:00:00 +0900
categories: review
# tags: [LSTM, Anomaly Detection, ICML, Deep Learning]
---
[Paper Link](https://arxiv.org/pdf/2101.00190)

# Abstract
Fine-tuning은 큰 사전 학습된 언어 모델(LM)을 활용하는 일반적인 방법이지만, 모든 LM 파라미터를 수정하므로 각 작업마다 전체 복사본을 저장해야 한다. 이 논문은 자연어 생성(NLG) 작업을 위한 fine-tuning의 가벼운 대안인 prefix-tuning을 제안한다. prefix-tuning은 LM 파라미터를 고정하고 작은 연속적인 작업별 벡터(prefix)를 최적화한다. prefix-tuning은 프롬프트에서 영감을 얻었으며, 이후 토큰들이 마치 "가상 토큰"인 것처럼 이 prefix에 주목할 수 있도록 한다. Prefix-tuning을 테이블-텍스트 생성에는 GPT-2에, 요약에는 BART에 적용한다. 0.1%의 파라미터만 학습함으로써, prefix-tuning은 전체 데이터 설정에서 fine-tuning과 비슷한 성능을 얻고, 적은 데이터 설정에서는 fine-tuning보다 우수하며, 학습 중 보지 못한 주제의 예시에도 더 좋은 일반화를 보인다.

# Introduction
![image](/images/prefix-tuning review/prefix-tuning review_2.png)
![image](/images/prefix-tuning review/prefix-tuning review_4.png)
![image](/images/prefix-tuning review/prefix-tuning review_5.png)![image](/images/prefix-tuning review/prefix-tuning review_6.png)
![image](/images/prefix-tuning review/prefix-tuning review_7.png)![image](/images/prefix-tuning review/prefix-tuning review_8.png)
![image](/images/prefix-tuning review/prefix-tuning review_9.png)![image](/images/prefix-tuning review/prefix-tuning review_10.png)![image](/images/prefix-tuning review/prefix-tuning review_11.png)
![image](/images/prefix-tuning review/prefix-tuning review_14.png)

# Methods
![image](/images/prefix-tuning review/prefix-tuning review_15.png)![image](/images/prefix-tuning review/prefix-tuning review_16.png)![image](/images/prefix-tuning review/prefix-tuning review_17.png)![image](/images/prefix-tuning review/prefix-tuning review_18.png)

# Experiments
![image](/images/prefix-tuning review/prefix-tuning review_19.png)
![image](/images/prefix-tuning review/prefix-tuning review_20.png)![image](/images/prefix-tuning review/prefix-tuning review_21.png)![image](/images/prefix-tuning review/prefix-tuning review_22.png)![image](/images/prefix-tuning review/prefix-tuning review_23.png)
![image](/images/prefix-tuning review/prefix-tuning review_24.png)![image](/images/prefix-tuning review/prefix-tuning review_25.png)![image](/images/prefix-tuning review/prefix-tuning review_26.png)![image](/images/prefix-tuning review/prefix-tuning review_27.png)![image](/images/prefix-tuning review/prefix-tuning review_28.png)![image](/images/prefix-tuning review/prefix-tuning review_29.png)![image](/images/prefix-tuning review/prefix-tuning review_30.png)![image](/images/prefix-tuning review/prefix-tuning review_31.png)![image](/images/prefix-tuning review/prefix-tuning review_32.png)![image](/images/prefix-tuning review/prefix-tuning review_33.png)![image](/images/prefix-tuning review/prefix-tuning review_34.png)![image](/images/prefix-tuning review/prefix-tuning review_35.png)

# Conclusion and Review
* Prefix-tuning 방법을 통한 통합 모델 제시
    * Prefix를 discrete vectors 대신 continuous한 vectors로 표현
* 기존의 lightweight tuing 방식보다 modularity가 더 큼
    * 전체 모델 대신 prefix vectors만을 테스크에 따라 다르게 학습
    * 앙상블의 효과를 얻을 수 있음
* Generation 테스크에서 prefix-tuning은 fine-tuning과 비교할 만한 성능을 보임
    * 특히 low-data settings에서 강건한 모습
* 간단한 방식과 적은 파라미터로 현재 일반적으로 사용되는 fine-tuning과 비슷한 성능을 보엿다는 것이 획기적임
    * Prompt를 어떻게 설계하는지가 중요
* 사전학습 때의 일반적인 지식을 fine-tuning하여 분포가 바뀌는 것이, 오히려 안좋을 수도 있다는 개인적인 생각
    * 인간에 가까운 인공지능 모델이 되려면 일반적인 지식으로 사람과 비슷한 판단을 내릴 수 있도록 하는 것이 좋을 것 같음