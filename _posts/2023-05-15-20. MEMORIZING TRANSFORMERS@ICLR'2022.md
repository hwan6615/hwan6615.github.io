---
layout: post
title: "20. Paper Review: MEMORIZING TRANSFORMERS@ICLR'2022"
# date: 2016-06-19 10:00:00 +0900
categories: review
# tags: [LSTM, Anomaly Detection, ICML, Deep Learning]
---
[Paper Link](https://arxiv.org/pdf/2303.07295)

# Abstract
이 논문은 새로운 지식을 학습하기 위해 가중치를 업데이트해야 하는 기존 언어 모델과 달리, 추론 시점에 새로운 데이터를 읽고 기억하여 즉시 지식을 습득할 수 있는 언어 모델을 제안한다. 이 연구는 과거 입력의 내부 표현을 기억하는 능력을 언어 모델에 추가한다.

이 모델은 최근 (키, 값) 쌍의 미분 불가능한 메모리에 대한 근사 kNN(k-Nearest Neighbor) 조회를 통해 C4, arXiv, PG-19, Github, Isabelle 등 다양한 벤치마크 및 작업에서 언어 모델링 성능을 향상시킨다. 메모리 크기를 최대 262K 토큰까지 늘리면 성능이 꾸준히 향상됨을 보여준다. 또한, 코드 및 수학 벤치마크에서 모델이 테스트 시점에 새로 정의된 함수와 정리를 활용할 수 있음을 확인한다.

# Introduction
![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_2.png)
![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_3.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_4.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_5.png)

# Proposed Methods
![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_6.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_7.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_8.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_9.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_10.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_11.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_12.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_13.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_14.png)

# Experiments
![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_15.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_16.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_17.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_18.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_19.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_20.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_21.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_22.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_23.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_24.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_25.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_26.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_27.png)![image](/images/MEMORIZING TRANSFORMERS/MEMORIZING TRANSFORMERS_28.png)

# Conclusion and Review
* Transformer 구조의 simple extension을 제안
	* Context length를 증가시키는 kNN-augmented attention
	* 다양한 language modeling 실험에서 external memory의 효과를 증명
* Memorizing Transformer는 모든 dataset과 architectures에서 perplexity의 향상을 보임
* Transformer의 size가 200M에서 8B까지 커질 때 external memory의 일관성 있는 성능 향상
* Pre-train을 scratch부터 시킬 필요가 없음
	* using existing PLM then fine-tuning
