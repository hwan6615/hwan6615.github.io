---
layout: post
title: "12. Paper Review: UNILMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training@ICML'2020"
# date: 2016-06-19 10:00:00 +0900
categories: review
# tags: [LSTM, Anomaly Detection, ICML, Deep Learning]
---
[Paper Link](https://proceedings.mlr.press/v119/bao20a/bao20a.pdf)

# Abstract
이 논문은 PMLM(Pseudo-Masked Language Model)이라는 새로운 훈련 절차를 사용하여 오토인코딩(autoencoding) 및 부분적으로 자기회귀적인(partially autoregressive) 언어 모델링 작업을 위한 통합 언어 모델을 사전 훈련하는 방법을 제안한다. 주요 내용은 아래와 같다.
-   **통합 모델**: PMLM은 오토인코딩(AE)을 통해 손상된 토큰과 컨텍스트 간의 관계를 학습하고, 부분 자기회귀 모델링(PAR)을 통해 마스크된 스팬(masked spans) 간의 관계를 학습하여 언어 이해와 생성 작업을 모두 수행할 수 있는 통합 언어 모델을 사전 훈련한다.

-   **효율성**: 잘 설계된 위치 임베딩과 자체 주의 마스크를 통해 컨텍스트 인코딩을 재사용하여 중복 계산을 피하고 계산 효율성을 높인다.

-   **성능**: PMLM을 사용하여 사전 훈련된 통합 언어 모델은 다양한 언어 이해 및 생성 작업에서 최신 기술(state-of-the-art) 결과를 달성하였다.

# Introduction
![image](/images/UNILMV2 review/UNILMV2 review_3.png)
![image](/images/UNILMV2 review/UNILMV2 review_4.png)![image](/images/UNILMV2 review/UNILMV2 review_5.png)![image](/images/UNILMV2 review/UNILMV2 review_6.png)![image](/images/UNILMV2 review/UNILMV2 review_7.png)
# Method
![image](/images/UNILMV2 review/UNILMV2 review_8.png)
![image](/images/UNILMV2 review/UNILMV2 review_9.png)![image](/images/UNILMV2 review/UNILMV2 review_10.png)![image](/images/UNILMV2 review/UNILMV2 review_11.png)![image](/images/UNILMV2 review/UNILMV2 review_13.png)
![image](/images/UNILMV2 review/UNILMV2 review_14.png)![image](/images/UNILMV2 review/UNILMV2 review_15.png)![image](/images/UNILMV2 review/UNILMV2 review_16.png)![image](/images/UNILMV2 review/UNILMV2 review_17.png)![image](/images/UNILMV2 review/UNILMV2 review_18.png)![image](/images/UNILMV2 review/UNILMV2 review_19.png)![image](/images/UNILMV2 review/UNILMV2 review_20.png)![image](/images/UNILMV2 review/UNILMV2 review_21.png)![image](/images/UNILMV2 review/UNILMV2 review_22.png)![image](/images/UNILMV2 review/UNILMV2 review_23.png)![image](/images/UNILMV2 review/UNILMV2 review_24.png)![image](/images/UNILMV2 review/UNILMV2 review_25.png)![image](/images/UNILMV2 review/UNILMV2 review_26.png)![image](/images/UNILMV2 review/UNILMV2 review_27.png)![image](/images/UNILMV2 review/UNILMV2 review_28.png)![image](/images/UNILMV2 review/UNILMV2 review_29.png)![image](/images/UNILMV2 review/UNILMV2 review_30.png)![image](/images/UNILMV2 review/UNILMV2 review_31.png)![image](/images/UNILMV2 review/UNILMV2 review_32.png)![image](/images/UNILMV2 review/UNILMV2 review_33.png)![image](/images/UNILMV2 review/UNILMV2 review_34.png)![image](/images/UNILMV2 review/UNILMV2 review_35.png)![image](/images/UNILMV2 review/UNILMV2 review_36.png)![image](/images/UNILMV2 review/UNILMV2 review_37.png)
# Experiments
![image](/images/UNILMV2 review/UNILMV2 review_38.png)![image](/images/UNILMV2 review/UNILMV2 review_39.png)![image](/images/UNILMV2 review/UNILMV2 review_40.png)![image](/images/UNILMV2 review/UNILMV2 review_41.png)![image](/images/UNILMV2 review/UNILMV2 review_42.png)![image](/images/UNILMV2 review/UNILMV2 review_43.png)![image](/images/UNILMV2 review/UNILMV2 review_44.png)![image](/images/UNILMV2 review/UNILMV2 review_45.png)![image](/images/UNILMV2 review/UNILMV2 review_46.png)
# Conclusion and Review
* Bidirectional, sequence-to-sequence LM을 통한 unified 사전학습
* Pseudo-masked LM을 통하여 사전학습 procedure를 제안
* 중복되는 계산 없이 one forward pass로 서로 다른 objectives 학습
	* Computationally efficient
* AE의 conventional masks는 global masking information을 학습, PAR은 masked spans 사이의 intra-relations를 학습
	* 두 modeling tasks는 서로 보완적
* 기존 unified LM의 문제점을 보완하기 위해서 사전학습 관점에서 창의적으로 해결
	* One forward pass를 적용하기 위한 pseudo mask 사용
	* 단일 모델로 exposure bias도 해결
* Multitask style의 사전학습을 pseudo mask를 추가하면서 가능하게 만듦
* 확실히 pre-training 관점에서 multitask style의 학습을 진행하는 것이 downstream task에 도움을 줌


